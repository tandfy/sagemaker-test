{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概要\n",
    "\n",
    "- 目的\n",
    "    - AWS IoT Analyticsの理解を深める\n",
    "    - Amazon SageMakerで異常検出モデル（ランダムカットフォレスト）をAWS IoT Analyticsのデータセットを使って学習する\n",
    "- やること\n",
    "    - AWS IoT Analyticsでチャネル、データストア、パイプライン、データセット(学習用とテスト用)を作成\n",
    "    - 検証用データセットのデータをチャネルに入れる\n",
    "    - データセットの作成処理を実行(クエリ実行)\n",
    "    - ノートブックで学習用とテスト用データセットを読み込む\n",
    "    - ランダムカットフォレストで異常値検出モデルを作る\n",
    "    - 結果確認\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ノートブック実行に必要なパッケージをインストール\n",
    "\n",
    "ノートブックを分けて各処理を書いているため、別のノートブックを実行することができるPapermillを使う。\n",
    "\n",
    "- <a target=\"_blank\" href=\"https://github.com/nteract/papermill\">nteract/papermill: 📚 Parameterize, execute, and analyze notebooks</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install papermill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータ定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "# 処理ノートブック\n",
    "notebook_dir = './notebook/'\n",
    "# papermillでノートブックを実行した場合の出力結果があるノートブック\n",
    "output_notebook_dir = './output_notebook/'\n",
    "\n",
    "\n",
    "# 対象のURLを指定\n",
    "data_source = 'https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/machine_temperature_system_failure.csv'\n",
    "label_data_source = 'https://raw.githubusercontent.com/numenta/NAB/master/labels/raw/known_labels_v1.0.json'\n",
    "\n",
    "# s3にデータを保存する場所定義\n",
    "prefix = 'machine_temperature_iot'\n",
    "bucket_name = 'bucket-name'\n",
    "\n",
    "# モデル学習ジョブ名\n",
    "base_job_name = 'machine-temperature-iot'\n",
    "\n",
    "# モデル学習周辺でのパス定義\n",
    "working_dir = './working/'\n",
    "train_dataset_path = path.join(working_dir, 'train_dataset.csv')\n",
    "test_dataset_path = path.join(working_dir, 'test_dataset.csv')\n",
    "train_data_s3_path = path.join('s3://', bucket_name, prefix, 'train.csv')\n",
    "test_data_s3_path = path.join('s3://', bucket_name, prefix, 'test.csv')\n",
    "model_artifact_path = path.join('s3://', bucket_name, prefix)\n",
    "transformed_data_s3_path = path.join('s3://', bucket_name, prefix)\n",
    "\n",
    "# AWS IoT Analytics関連のリソース名\n",
    "channel_name = prefix\n",
    "pipeline_name = prefix\n",
    "datastore_name = prefix\n",
    "train_dataset_name = prefix+'_train'\n",
    "test_dataset_name = prefix+'_test'\n",
    "\n",
    "# データのシングリングサイズ(１つのデータポイントの長さ)\n",
    "shingle_size = 12*24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前準備\n",
    "## AWS IoT Analyticsのリソース作成\n",
    "\n",
    "- チャネル、データストア、パイプライン、データセット\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_resource_parameters = dict(\n",
    "    channel_name = channel_name,\n",
    "    pipeline_name = pipeline_name,\n",
    "    datastore_name = datastore_name,\n",
    "    train_dataset_name = train_dataset_name,\n",
    "    test_dataset_name = test_dataset_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "from os import path\n",
    "exec_notebook_name = 'create_resource.ipynb'\n",
    "pm.execute_notebook(\n",
    "    path.join(notebook_dir, exec_notebook_name),\n",
    "    path.join(output_notebook_dir, exec_notebook_name),\n",
    "    parameters = create_resource_parameters,\n",
    "    kernel_name ='python3'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS IoT Analyticsのチャネルへデータを入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put_data_parameters = dict(\n",
    "    channel_name = channel_name,\n",
    "    csv_data_source = data_source,\n",
    "    data_split_size = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "from os import path\n",
    "exec_notebook_name = 'put_data.ipynb'\n",
    "pm.execute_notebook(\n",
    "    path.join(notebook_dir, exec_notebook_name),\n",
    "    path.join(output_notebook_dir, exec_notebook_name),\n",
    "    parameters = put_data_parameters,\n",
    "    kernel_name ='python3'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの作成\n",
    "\n",
    "データストアからデータを抽出してデータセットを作る。  \n",
    "実行してから数分後にデータセットの作成が完了する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "iota_client = boto3.client('iotanalytics')\n",
    "for dataset_name in [train_dataset_name, test_dataset_name]:\n",
    "    response = iota_client.create_dataset_content(datasetName=dataset_name)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 異常検出モデルの作成\n",
    "## 学習用とテスト用データセットをダウンロード\n",
    "*データセットの作成が終わっていない場合はエラーが出るので注意*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = [\n",
    "    dict(\n",
    "        dataset_name = train_dataset_name,\n",
    "        file_path = train_dataset_path\n",
    "    ),\n",
    "    dict(\n",
    "        dataset_name = test_dataset_name,\n",
    "        file_path = test_dataset_path\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "iota_client = boto3.client('iotanalytics')\n",
    "\n",
    "for dataset_param in dataset_params:\n",
    "    # データセットの情報を取得\n",
    "    response = iota_client.get_dataset_content(datasetName = dataset_param['dataset_name'])\n",
    "    # データセットの署名つきURLを取りだす\n",
    "    data_uri = response['entries'][0]['dataURI']\n",
    "    \n",
    "    # ファイルデータを取り出す\n",
    "    urlretrieve(data_uri, dataset_param['file_path'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data_parameters_list = [('train', dict(\n",
    "    dataset_path = train_dataset_path,\n",
    "    data_s3_path = train_data_s3_path,\n",
    "    label_data_source = '',\n",
    "    shingle_size = shingle_size,\n",
    ")),('test', dict(\n",
    "    dataset_path = test_dataset_path,\n",
    "    data_s3_path = test_data_s3_path,\n",
    "    label_data_source = label_data_source,\n",
    "    shingle_size = shingle_size,\n",
    "))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "from os import path\n",
    "exec_notebook_name = 'preprocess_data.ipynb'\n",
    "for phase_name, preprocess_data_parameters in preprocess_data_parameters_list:\n",
    "    pm.execute_notebook(\n",
    "        path.join(notebook_dir, exec_notebook_name),\n",
    "        path.join(output_notebook_dir, phase_name+'_'+exec_notebook_name),\n",
    "        parameters = preprocess_data_parameters,\n",
    "        kernel_name ='python3'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "SageMakerで異常検出(ランダムカットフォレスト)モデルを学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "hyperparameters = dict(\n",
    "    num_samples_per_tree=256,\n",
    "    num_trees=100,\n",
    "    feature_dim=shingle_size\n",
    ")\n",
    "train_parameters = dict(\n",
    "    train_s3_path = train_data_s3_path,\n",
    "    test_s3_path = test_data_s3_path,\n",
    "    execution_role = sagemaker.get_execution_role(),\n",
    "    hyperparameters = hyperparameters,\n",
    "    model_artifact_path = model_artifact_path,\n",
    "    base_job_name = base_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "from os import path\n",
    "exec_notebook_name = 'train.ipynb'\n",
    "pm.execute_notebook(\n",
    "    path.join(notebook_dir, exec_notebook_name),\n",
    "    path.join(output_notebook_dir, exec_notebook_name),\n",
    "    parameters = train_parameters,\n",
    "    kernel_name ='python3'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習ジョブ名を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = pm.read_notebook(path.join(output_notebook_dir, exec_notebook_name))\n",
    "training_job_name = nb.data['job_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirm_model_parameters = dict(\n",
    "    training_job_name = training_job_name,\n",
    "    labeled_test_data_s3_path = test_data_s3_path,\n",
    "    output_data_s3_path = transformed_data_s3_path,\n",
    "    shingle_size = shingle_size    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "from os import path\n",
    "exec_notebook_name = 'confirm_model.ipynb'\n",
    "pm.execute_notebook(\n",
    "    path.join(notebook_dir, exec_notebook_name),\n",
    "    path.join(output_notebook_dir, exec_notebook_name),\n",
    "    parameters = confirm_model_parameters,\n",
    "    kernel_name ='python3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = 's3://osawa-test-ml/machine_temperature_iot/test_transform.csv'\n",
    "import pandas as pd\n",
    "df = pd.read_csv(s3path, header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# リソースの削除\n",
    "IoT Analytics関連のリソースを削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_resource_parameters = dict(\n",
    "    channel_name = channel_name,\n",
    "    pipeline_name = pipeline_name,\n",
    "    datastore_name = datastore_name,\n",
    "    train_dataset_name = train_dataset_name,\n",
    "    test_dataset_name = test_dataset_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "from os import path\n",
    "exec_notebook_name = 'delete_resource.ipynb'\n",
    "pm.execute_notebook(\n",
    "    path.join(notebook_dir, exec_notebook_name),\n",
    "    path.join(output_notebook_dir, exec_notebook_name),\n",
    "    parameters = delete_resource_parameters,\n",
    "    kernel_name ='python3'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
